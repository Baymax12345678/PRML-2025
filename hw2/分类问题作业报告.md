# 分类问题作业报告

## Abstract

本次作业报告是对课上所学的决策树、集成学习和核方法部分知识的巩固和应用，旨在利用 Decision Trees、AdaBoost + Decision Trees 和 SVM 三种方法来对一个 3D 数据集进行分类，在复习相关知识的同时也锻炼了调整超参数的能力，以及分析在不同任务中选择合适分类算法的能力。

## Introduction

决策树是一种基于树形结构的监督学习算法，用于分类或回归任务。它通过递归地将数据集按照特征进行分裂，形成一系列的“决策节点”和“叶节点”，最终构建一棵树形模型。其直观易懂，易于解释，对数据预处理要求低，能够处理非线性关系，并且训练速度快，适合小到中等规模的数据集。但容易过拟合且对数据分布敏感。

AdaBoost 是一种集成学习算法，通过组合多个弱学习器（如浅层决策树）来构建一个强学习器。其具有一定的鲁棒性和泛化能力，但对异常值敏感且训练过程较慢。

SVM 是一种基于几何直观的监督学习算法，用于分类或回归任务。其目标是找到一个超平面，使得不同类别的样本在该超平面上的间隔最大化。其可以通过核技巧避免维度灾难，对高维数据表现良好，但计算复杂度高，参数选择也较复杂。

本次作业中使用了 Decision Trees、AdaBoost + Decision Trees 和 SVM 三种方法来对给定高维数据集进行二分类，同时比较三种方法的分类性能。而由于该数据为三维数据，预测使用合适核方法的 SVM 算法可能在此任务中表现最好。

## Methodology

### 1. 决策树

决策树是一种常用于机器学习和数据挖掘的强大工具，可用于分类和回归任务。它是一种树形结构的模型，其中内部节点表示对属性的测试，分支表示这些测试的结果，叶节点表示决策结果或类标签。从根到叶的路径表示分类规则或回归路径。

决策树算法首先选取最具区分性的属性作为分类的基础，随后利用区分性较小的属性进行进一步划分。而信息增益就是决策树用来衡量一个属性的区分性的指标。一个概率分布的信息熵可通过下式计算：

$$
Entropy(X) = -\sum_X P(X)\log(P(X))
$$

一个属性的信息增益则可表示为：

$$
Gain(S,A)=Entropy(S)-\sum_{v\in Values(A)} \frac{|S_v|}{|S|}Entropy(S_v)
$$

计算每一个属性的信息增益，最终可选取信息增益最大的属性作为决策树的分类节点，并不断循环这一过程直到所有属性都加到树上。

在实验中则可以通过直接调用 `sklearn` 库来使用决策树算法，其中可以设置 `max_depth` 和 `random_state`，`random_state` 通常被设置为 42。

### 2. AdaBoost

AdaBoost（Adaptive Boosting）是一种经典的集成学习算法，通过组合多个弱分类器（如决策树桩）构建一个强分类器。其核心思想是迭代训练多个弱学习器，每轮训练时增加错分样本的权重，使得下一轮训练更关注于难以分类的样本，进而提高分类器整体性能。

AdaBoost 算法的具体步骤如下：

1. 初始化样本权重，使所有样本权重相等。
2. 循环进行以下步骤直至达到设定的分类器数量：
    - 基于当前样本权重训练一个弱分类器。
    - 计算该弱分类器的错误率。
    - 根据错误率更新弱分类器的权重（分类性能越好，权重越大）。
    - 更新样本权重，增大被错误分类样本的权重，减小正确分类样本的权重。
3. 结合所有弱分类器，按其权重加权投票，得到最终的强分类器。

在实验中，使用 scikit-learn 的 `AdaBoostClassifier` 实现 AdaBoost 算法，通过调整 `n_estimators`（弱分类器数量）和 `learning_rate`（学习速率）等超参数来提高性能。

### 3. 支持向量机（SVM）

支持向量机（Support Vector Machine, SVM）是一种经典的监督学习算法，广泛用于分类和回归问题。其核心思想是在特征空间中寻找一个最优的分割超平面，以最大化不同类别之间的间隔，从而提高泛化性能。

SVM 的关键概念包括：

- **支持向量**：最靠近分割超平面的样本，这些样本决定了超平面的方向和位置。
- **最大间隔超平面**：通过最大化分类间隔，提升模型对未知数据的鲁棒性。
- **核函数（Kernel Function）**：在非线性数据中，通过将数据映射到高维空间，使得非线性可分的数据变为线性可分，常见核函数包括线性核、多项式核、高斯径向基核（RBF）等。

在实验中，可以使用 scikit-learn 的 `SVC` 实现 SVM，通常采用 RBF 核函数，并通过调整 `C`（惩罚参数）和 `gamma`（核系数）等超参数获得最佳分类性能。

## Experimental Studies

将这三种方法在不同的测试集上每种运行十次，并将所得准确率取平均值，在该3D数据测试集上的分类表现如下表所示：

| 分类器          | 准确率 |
| --------------- | ------ |
| 决策树          | 0.956  |
| AdaBoost+决策树 | 0.980  |
| SVM（线性核）   | 0.674  |
| SVM（多项式核） | 0.872  |
| SVM（rbf核）    | 0.986  |

该数据集呈现出三维“月亮”形状的分布，这类分布具有明显的非线性特征，并且边界较为复杂。通过表格结果可见，采用 RBF 核函数的 SVM 算法在分类任务中取得了最高的准确率，这与我们在实验前的预期是一致的。RBF 核的优势在于它能够将数据映射到高维空间，适合处理非线性问题，从而更有效地拟合复杂边界。

排名第二的是结合了 AdaBoost 与决策树的算法。AdaBoost 属于集成学习方法，它通过不断调整样本权重、提升弱分类器性能，从而增强整体模型的泛化能力。与决策树结合后，能够更精细地刻画数据的复杂结构，尤其是在边界复杂或分布不均的区域表现优异，同时也能一定程度上抑制过拟合。

第三名是单独使用的决策树算法。由于决策树通过特征的分裂来逐步构建决策边界，因此天然适合处理非线性问题。在这种“月亮”形状的三维分布中，决策树能够通过多次划分来逼近复杂边界。但其缺点是，面对高维数据时较容易发生过拟合。

而表现一般的是使用多项式核函数的 SVM。虽然多项式核可以处理非线性问题，但其效果高度依赖于多项式的阶数。对于这种形状复杂的三维数据，多项式核可能无法精准捕捉其边界特征，因此表现略逊于 RBF 核。

## 总结

本次实验中，我们分别应用了决策树、AdaBoost 和 SVM 三种经典的机器学习分类方法，对给定的 3D 数据集进行了分类任务。

- 决策树方法简单易懂，直观性强，但在复杂数据集中容易出现过拟合。
- AdaBoost 通过集成多个决策树桩，在泛化性能和鲁棒性上都有明显提升，但对异常值敏感。
- SVM 通过核技巧处理非线性数据，具有较好的泛化能力，尤其适合于本次任务中三维数据的分类。

综合实验表现，SVM 凭借其优秀的分类精度和对高维数据的高效处理能力，表现最佳，适合作为复杂分类任务的首选模型。
